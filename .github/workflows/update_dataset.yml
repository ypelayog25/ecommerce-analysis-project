name: Update Dataset and Run Analysis

on:
  schedule:
    - cron: "0 0 * * *"  # Automatically runs daily at midnight UTC
  workflow_dispatch:
    inputs:
      FORCE_RUN:
        description: "Force full analysis (ignore hash check)"
        required: false
        default: "true"

jobs:
  update-and-analyze:
    runs-on: ubuntu-latest

    env:
      KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
      KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
      FORCE_RUN: ${{ github.event.inputs.FORCE_RUN }}

    steps:
      # âœ… Step 1: Checkout repository
      - name: Checkout Repository
        uses: actions/checkout@v3
        with:
          lfs: true

      # âœ… Step 2: Setup Python environment
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      # âœ… Step 3: Install Python dependencies
      - name: Install Kaggle and dependencies
        run: |
          pip install kaggle pandas matplotlib pyarrow

      # âœ… Step 4: Configure Kaggle API credentials
      - name: Configure Kaggle
        run: |
          mkdir -p ~/.kaggle
          echo "{\"username\":\"$KAGGLE_USERNAME\", \"key\":\"$KAGGLE_KEY\"}" > ~/.kaggle/kaggle.json
          chmod 600 ~/.kaggle/kaggle.json

      # âœ… Step 5: Ensure raw data directory exists
      - name: Ensure raw data folder exists
        run: mkdir -p data/raw

      # âœ… Step 6: Download latest dataset metadata
      - name: Download dataset metadata
        run: kaggle datasets metadata nabihazahid/ecommerce-dataset-for-sql-analysis -p data/raw

      # âœ… Step 7: Detect changes using hash OR force run if manual trigger
      - name: Detect dataset changes
        id: hash-check
        run: |
          cd data/raw
          NEW_HASH=$(sha256sum dataset-metadata.json | cut -d " " -f1)
          if [ -f .dataset_hash.txt ]; then
            OLD_HASH=$(cat .dataset_hash.txt)
          else
            OLD_HASH=""
          fi
          echo "OLD_HASH: $OLD_HASH"
          echo "NEW_HASH: $NEW_HASH"

          if [[ "$FORCE_RUN" == "true" ]]; then
            echo "FORCE_RUN enabled â€” full analysis will run"
            echo "::set-output name=changed::true"
          elif [[ "$OLD_HASH" != "$NEW_HASH" ]]; then
            echo "New dataset detected â€” updating hash"
            echo "$NEW_HASH" > .dataset_hash.txt
            echo "::set-output name=changed::true"
          else
            echo "No dataset changes detected â€” skipping analysis"
            echo "::set-output name=changed::false"
          fi

      # âœ… Step 8: Download dataset only if updated or force mode enabled
      - name: Download dataset if changed
        if: steps.hash-check.outputs.changed == 'true'
        run: kaggle datasets download -d nabihazahid/ecommerce-dataset-for-sql-analysis -p data/raw --unzip

      # âœ… Step 9: Run your analysis (Python script)
      - name: Run analysis script
        if: steps.hash-check.outputs.changed == 'true'
        run: |
          echo "ðŸš€ Running full data analysis..."
          python src/data/step5_analysis_sql.py

      # âœ… Step 10: Commit and push updated reports
      - name: Commit and push new hash & reports
        if: steps.hash-check.outputs.changed == 'true'
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          git add data/raw/.dataset_hash.txt reports/
          git commit -m "chore: update reports after dataset analysis"
          git push  
