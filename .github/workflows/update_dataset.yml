name: Update Dataset and Run Analysis

on:
  schedule:
    - cron: "0 0 * * *"  # Runs daily at midnight UTC
  workflow_dispatch:
    inputs:
      FORCE_RUN:
        description: "Force full analysis (ignore hash check)"
        required: false
        default: "true"

jobs:
  update-and-analyze:
    runs-on: ubuntu-latest

    env:
      KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
      KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
      FORCE_RUN: ${{ github.event.inputs.FORCE_RUN }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          lfs: true

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install kaggle pandas matplotlib duckdb pyarrow

      - name: Configure Kaggle
        run: |
          mkdir -p ~/.kaggle
          echo "{\"username\":\"${{ secrets.KAGGLE_USERNAME }}\", \"key\":\"${{ secrets.KAGGLE_KEY }}\"}" > ~/.kaggle/kaggle.json
          chmod 600 ~/.kaggle/kaggle.json

      - name: Create raw data directory
        run: mkdir -p data/raw

      - name: Download dataset metadata
        run: kaggle datasets metadata nabihazahid/ecommerce-dataset-for-sql-analysis -p data/raw

      - name: Detect dataset changes or force run
        id: hash-check
        run: |
          cd data/raw
          if [ -f dataset-metadata.json ]; then
            NEW_HASH=$(sha256sum dataset-metadata.json | cut -d " " -f1)
          else
            echo "❌ ERROR: dataset-metadata.json not found"
            exit 1
          fi

          if [ -f .dataset_hash.txt ]; then
            OLD_HASH=$(cat .dataset_hash.txt)
          else
            OLD_HASH=""
          fi

          echo "OLD_HASH=$OLD_HASH"
          echo "NEW_HASH=$NEW_HASH"

          if [[ "$FORCE_RUN" == "true" ]]; then
            echo "⚠️ FORCE RUN enabled — skipping hash check"
            echo "::set-output name=changed::true"
          elif [[ "$NEW_HASH" != "$OLD_HASH" ]]; then
            echo "✅ Dataset has changed"
            echo "::set-output name=changed::true"
          else
            echo "ℹ️ No changes detected — skipping analysis"
            echo "::set-output name=changed::false"
          fi

      - name: Download dataset if changed or forced
        if: steps.hash-check.outputs.changed == 'true'
        run: kaggle datasets download -d nabihazahid/ecommerce-dataset-for-sql-analysis -p data/raw --unzip

      - name: Run analysis script
        if: steps.hash-check.outputs.changed == 'true'
        run: |
          echo "Running analysis..."
          python src/step5_analysis_sql.py

      - name: Ensure dataset hash file exists
        run: |
          if [ ! -f "data/raw/.dataset_hash.txt" ]; then
            echo "INITIAL_RUN" > data/raw/.dataset_hash.txt
          fi

      - name: Save new dataset hash
        if: steps.hash-check.outputs.changed == 'true'
        run: |
          NEW_HASH=$(sha256sum data/raw/dataset-metadata.json | cut -d " " -f1)
          echo "$NEW_HASH" > data/raw/.dataset_hash.txt

      - name: Commit and push updates
        if: steps.hash-check.outputs.changed == 'true'
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          git add data/raw/.dataset_hash.txt reports/
          git commit -m "chore: update reports after dataset analysis"
          git push
